{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7814de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalbns/miniforge3/envs/mldl/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script fine-tunes a sequence classification model using a dataset of patent descriptions. \n",
    "It employs the Llama-3.2-1B model with PEFT (Parameter-Efficient Fine-Tuning) using LoRA (Low-Rank Adaptation) to \n",
    "reduce the number of trainable parameters from 100% to 0.07%, improving computational efficiency. \n",
    "The dataset is preprocessed and tokenized for training and evaluation. \n",
    "LoRA configuration is applied to specific model layers, enabling lightweight fine-tuning. \n",
    "Metrics like accuracy and F1-score are computed to evaluate model performance. \n",
    "The script uses the Hugging Face Trainer API for training and evaluation, logs progress to TensorBoard, \n",
    "and saves the fine-tuned model for deployment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00fada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Human Necessities', 'Performing Operations; Transporting', 'Chemistry; Metallurgy', 'Textiles; Paper', 'Fixed Constructions', 'Mechanical Engineering; Lightning; Heating; Weapons; Blasting', 'Physics', 'Electricity', 'General tagging of new or cross-sectional technology']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70661efa23584505a753f95fb639dad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7a9418b048487caf1ad691f62ea9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620913595f274bd18442c63e2a4e0c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "DATA PREP\n",
    "\n",
    "'''\n",
    "\n",
    "ds = load_dataset(\"ccdv/patent-classification\", \"patent\")\n",
    "\n",
    "# Print the unique labels in the dataset\n",
    "label_names = ds[\"train\"].features[\"label\"].names\n",
    "print(label_names)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "# Apply tokenization to the entire dataset\n",
    "encoded_dataset = ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the structure of the dataset\n",
    "print(encoded_dataset)\n",
    "\n",
    "# Split the dataset into train and eval sets\n",
    "train_dataset = encoded_dataset[\"train\"].select(range(500))\n",
    "eval_dataset = encoded_dataset[\"validation\"].select(range(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef1585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "MODEL PREP\n",
    "\n",
    "'''\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# Change the architecture to LlamaForSequenceClassification\n",
    "config.architectures = [\"LlamaForSequenceClassification\"]\n",
    "\n",
    "# Add a fallback for rope_scaling attribute, if it exists\n",
    "if hasattr(config, \"rope_scaling\") and \"type\" not in config.rope_scaling:\n",
    "    config.rope_scaling[\"type\"] = \"default\"\n",
    "\n",
    "num_labels = len(ds[\"train\"].features[\"label\"].names)\n",
    "\n",
    "# Set num_labels in the config\n",
    "config.num_labels = num_labels\n",
    "\n",
    "# Load and configure the model\n",
    "label_names = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Update model configuration with label mappings\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.label2id = {label: idx for idx, label in enumerate(label_names)}\n",
    "config.id2label = {idx: label for idx, label in enumerate(label_names)}\n",
    "\n",
    "''' QUANTIZATION\n",
    "\n",
    "# Use BitsAndBytesConfig for 8-bit quantization\n",
    "from bitsandbytes import nn as bnb\n",
    "bnb_config = bnb.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Set this to True for 8-bit quantization\n",
    "    quantization_method=\"nf4\",  # You can use 'int4' or 'nf4' for quantization\n",
    ")\n",
    "'''\n",
    "# Load the sequence classification model\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,  # Pass the config with num_labels and updated architecture\n",
    "    torch_dtype=torch.float32,  # Use float16 precision for memory efficiency. does not work with cpu.\n",
    "    #quantization_config=bnb_config,\n",
    ").to(device)\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "# Set the padding token ID if not already set\n",
    "original_model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25bc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 1235832832\n",
      "all model parameters: 1235832832\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "PRINT NUMBER OF PARAMETERS TO TRAIN ORIGINAL MODEL\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to print trainable parameters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f08f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable model parameters: 851968\n",
      "all model parameters: 1236684800\n",
      "percentage of trainable model parameters: 0.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalbns/miniforge3/envs/mldl/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "LoRA\n",
    "\n",
    "'''\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,            # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA applies to these layers\n",
    "    lora_dropout=0.1,  # Dropout in LoRA layers\n",
    "    bias=\"none\",  # Bias term handling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the original model and move it to the appropriate device\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "peft_model.to(device)  # Ensure it's moved to the correct device\n",
    "peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "'''\n",
    "\n",
    "PRINT NUMBER OF PARAMETERS TO TRAIN LORA\n",
    "\n",
    "'''\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6fff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4d49da0e8ae4>:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Force model to use CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load model and move to the correct device\n",
    "original_model = original_model.to(device)\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "# Check which device it's on\n",
    "print(f\"Model is on {peft_model.device}\")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "SET MODEL TRAIN PARAMETERS\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "#Compute metrics function is not being called by trainer because the peft_model is used not original model. \n",
    "#It is a work in progress issue.\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"Inside compute_metrics function.\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)  # Convert logits to predicted labels\n",
    "    accuracy = accuracy_score(labels, predictions)  # Calculate accuracy\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")  # Calculate weighted F1 score\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=1,   # Batch size for training\n",
    "    per_device_eval_batch_size=1,    # Batch size for evaluation\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir=\"./logs\",            # Directory for storing logs\n",
    "    logging_steps=25,                # Frequency of logging\n",
    "    report_to=\"tensorboard\",         # Log to TensorBoard\n",
    "    eval_strategy=\"steps\",           # Evaluate every 10 steps\n",
    "    eval_steps=25,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_strategy=\"epoch\",           # Save only at the end of each epoch\n",
    "    save_total_limit=25,              # Keep only the last 2 checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,                    # The pre-trained model\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=eval_dataset,           # Evaluation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer\n",
    "    compute_metrics=compute_metrics,     # Custom metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eabff85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSAVE MODEL\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "TRAIN MODEL\n",
    "\n",
    "'''\n",
    "\n",
    "#trainer.train()\n",
    "#print(f\"Trainer: {trainer}\")\n",
    "'''\n",
    "\n",
    "EVALUATE MODEL\n",
    "\n",
    "'''\n",
    "\n",
    "#eval_results = trainer.evaluate()\n",
    "#print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "'''\n",
    "\n",
    "SAVE MODEL\n",
    "\n",
    "'''\n",
    "\n",
    "# Save the final model\n",
    "#trainer.save_model(\"../final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90ec5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
